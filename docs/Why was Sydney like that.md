---
share: true
title: Why was Sydney (Bing) like that?
---

There has been speculation and marvel at why Sydney was like that. Was it the prompt? The model? This notes briefly captures an answer and links to more in depth answers.

[This commentary from Janus on using GPT-4 Base](https://www.lesswrong.com/posts/tbJdxJMAiehewGpq2/impressions-from-base-gpt-4?commentId=CemTb7c2gmwuSAwCC) (2023-11-09) and [a response from Gwern](https://www.lesswrong.com/posts/tbJdxJMAiehewGpq2/impressions-from-base-gpt-4?commentId=ioZGEADiFKswoLCPP) paint a good picture of why. I recommend reading both in full.
> Gwern: This also sheds some light on [why Sydney](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K) (a snapshot of GPT-4-base partway through training) would disagree with the user so much or be so stubborn. It's not that the MS training was responsible, but more characteristic of the base model.

Gwern's [why Sydney](https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned?commentId=AAC8jKeDp6xqsZK2K) link goes to  comment of his from 2023-02-23 that goes deeper into understanding what model Sydney was based on.

See also
- https://cyborgism.wiki/hypha/bing